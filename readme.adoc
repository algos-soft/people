:doctype: article
:doctitle: People application
:!toc:

The application provides an endpoint to upload .csv files to a database and a simple monitoring UI.

It has been implemented as a SpringBoot application with open source libraries added to simplify some common tasks.

==== Libraries used
The main libraries added to SpringBoot are:

- *h2 database*, as the implementation of Spring JPA

- *vaadin*, in order to build a quick and simple UI

- *apache commons-csv*, for parsing the csv format

- *apache tika*, for mime type detection

- *okhttp* and *moshi*, for the sample data generator

==== Requirements:

JRE 8.x and Maven must be available

==== Installation:
- unzip the provided package.

- cd to the project directory and run:

....
mvn clean package -Pproduction
....

==== Running the application:
cd to the project directory and run:

....
java -jar target/people-1.0-SNAPSHOT.jar
....

(when the server is ready you can connect to the UI at http://localhost:8080)


==== Usage
The csv files are submitted via http requests to the endpoint http://localhost:8080/people/submit

example with curl:
....
curl --location --request POST http://localhost:8080/people/submit --form file=@<myfile.csv>
....

==== Notes

===== Persistence layer
The persistence implementation is based on the H2 database and Spring Data JPA (the persistence layer can easily be changed by providing different implementations of the Repository classes).

The application creates a H2 datafile in ~/people/

===== CSV implementation
CSV is an old format and comes in various flavors. Since the flavor was not specified, the standard format defined in RFC 4180 has been implemented (using commas as separators).
The parser used is Apache Commons CSV.

===== Email field uniqueness
Due to the uniqueness constraint on the email field, a unique index has been added at persistence level.

....
Person.class -> @Table(indexes = @Index(columnList = "email"))
....

Persistence exceptions are intercepted at Business level and Presentation level in order to manage the error condition appropriately at each level.

===== Date format problem
The guidelines didn't specify the format for the date field, so a brute force parser had to be  used in order to support different possible date formats.

A more efficient solution could be used if some specs on the date format were provided.

===== .txt file type problem
[quote]
"Given a new file to import, when I upload a *.txt file, then I should receive an error"

This requirement was a bit misleading.

Although is possible to detect the original filename, a .txt extension doesn't mean that the file does not contain a valid csv structure, and in the other hand a .csv extension does not guarantee to contain a valid csv content.

The check on the .txt extension has been implemented, but another check was added, which detects the MIME type of the content through Apache Tika, in order to refuse any non "text/plain" content.

===== Handling of invalid data
In case of malformed csv or missing email, the whole import operation is interrupted. Other strategies which could be implemented may include rolling back the transaction or skipping the invalid entries.

===== Error handling
In general, all the errors are logged at service level with detailed technical information and error stacktrace.

At higher level (controller level), the errors are catched, grouped and returned with a level of detail meaningful to the client application.

The client will then  decide how to notify the user with the appropriate level of detail.

===== Unit testing
Bla bla

===== Random user generator
In order to test the endpoint, some .csv files with random people and different sizes were needed, so a small service based on randomuser.me has been implemented to generate them.

You can access the generator service at http://localhost:8080/generator if you are interested.

===== Test data
In the *testfiles* directory you can find different data files for testing.

NOTE: Note: the 10.000 rows datafile does not produce 10.000 rows in the db because some email addresses are duplicated.
